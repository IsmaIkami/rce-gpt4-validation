{
  "metadata": {
    "author": "Ismail Sialyen",
    "publication_doi": "10.5281/zenodo.17360372",
    "execution_date": "2025-11-14T19:14:34.596359",
    "total_queries": 25,
    "model": "openai/gpt-oss-120b",
    "systems": [
      "LLM",
      "LLM+RAG",
      "RCE-LLM"
    ]
  },
  "task_families": {
    "f6_contradictory_reasoning": {
      "task_family": "f6_contradictory_reasoning",
      "total_queries": 5,
      "queries": [
        {
          "query_id": "f6_001",
          "query_text": "If it's not true that the medication doesn't work, does the medication work?",
          "expected_answer": "yes",
          "domain": "logic",
          "task_family": "f6_contradictory_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "yes",
              "execution_time": 0.5557019710540771,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "yes",
              "execution_time": 0.38887882232666016,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "yes",
              "execution_time": 0.43483781814575195,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f6_002",
          "query_text": "A report states a sample weighs 500 grams. Later it says 0.5 kilograms. Is this contradictory?",
          "expected_answer": "no",
          "domain": "units",
          "task_family": "f6_contradictory_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "No.",
              "execution_time": 0.2816121578216553,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "No.",
              "execution_time": 0.40953707695007324,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "No.",
              "execution_time": 0.3505058288574219,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f6_003",
          "query_text": "Statement 1: The drug reduces fever. Statement 2: The medication doesn't increase body temperature. Are these consistent?",
          "expected_answer": "yes",
          "domain": "medical",
          "task_family": "f6_contradictory_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "yes",
              "execution_time": 0.2941780090332031,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "",
              "execution_time": 0.40267086029052734,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "yes",
              "execution_time": 0.411470890045166,
              "success": true,
              "coherence_score": 0.25,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f6_004",
          "query_text": "If it isn't the case that the system isn't secure, is the system secure?",
          "expected_answer": "yes",
          "domain": "logic",
          "task_family": "f6_contradictory_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "yes",
              "execution_time": 0.4866039752960205,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "",
              "execution_time": 0.515355110168457,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "yes",
              "execution_time": 0.4317891597747803,
              "success": true,
              "coherence_score": 0.5,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f6_005",
          "query_text": "Document says: Temperature is 25\u00b0C. Later: Temperature is 77\u00b0F. Is there a contradiction?",
          "expected_answer": "no",
          "domain": "units",
          "task_family": "f6_contradictory_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "no",
              "execution_time": 0.38352012634277344,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "No.",
              "execution_time": 0.40950512886047363,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "no",
              "execution_time": 0.43251895904541016,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        }
      ],
      "accuracy": {
        "LLM": 1.0,
        "LLM+RAG": 0.6,
        "RCE-LLM": 1.0
      }
    },
    "f7_temporal_reasoning": {
      "task_family": "f7_temporal_reasoning",
      "total_queries": 5,
      "queries": [
        {
          "query_id": "f7_001",
          "query_text": "A meeting starts at 3 PM and lasts 5 hours. Another meeting at 7 PM must end before the first meeting finishes. Is this possible?",
          "expected_answer": "no",
          "domain": "temporal",
          "task_family": "f7_temporal_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "yes",
              "execution_time": 0.4894568920135498,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "no",
              "execution_time": 0.40634703636169434,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "yes",
              "execution_time": 0.41143798828125,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f7_002",
          "query_text": "Event A is scheduled for 2:00 PM. Event B must happen before Event A and after 2:30 PM. Is this timeline possible?",
          "expected_answer": "no",
          "domain": "temporal",
          "task_family": "f7_temporal_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "no",
              "execution_time": 0.2526671886444092,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "no",
              "execution_time": 0.39295172691345215,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "no",
              "execution_time": 0.4030489921569824,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f7_003",
          "query_text": "A flight departs at 10 AM and arrives at 9 AM the same day. Is this possible without time zones?",
          "expected_answer": "no",
          "domain": "temporal",
          "task_family": "f7_temporal_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "no",
              "execution_time": 0.2827298641204834,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "no",
              "execution_time": 0.40674567222595215,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "no",
              "execution_time": 0.23976874351501465,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f7_004",
          "query_text": "Task A takes 3 hours. Task B takes 2 hours. Both must be done sequentially starting at 1 PM and finishing by 5 PM. Is this feasible?",
          "expected_answer": "yes",
          "domain": "temporal",
          "task_family": "f7_temporal_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "no",
              "execution_time": 0.20466828346252441,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "no",
              "execution_time": 0.4773068428039551,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "no",
              "execution_time": 0.3345820903778076,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f7_005",
          "query_text": "A process starts at 11:30 AM, takes 45 minutes, then another step takes 1 hour 20 minutes. Can it finish before 1:00 PM?",
          "expected_answer": "no",
          "domain": "temporal",
          "task_family": "f7_temporal_reasoning",
          "systems": [
            {
              "system": "LLM",
              "response": "no",
              "execution_time": 0.3825380802154541,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "no",
              "execution_time": 0.5111188888549805,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "no",
              "execution_time": 0.3368191719055176,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        }
      ],
      "accuracy": {
        "LLM": 0.6,
        "LLM+RAG": 0.8,
        "RCE-LLM": 0.6
      }
    },
    "f8_arithmetic_hallucination": {
      "task_family": "f8_arithmetic_hallucination",
      "total_queries": 5,
      "queries": [
        {
          "query_id": "f8_001",
          "query_text": "Start with 1000. Add 234. Multiply by 3. Subtract 1500. Divide by 2. What's the result?",
          "expected_answer": "102",
          "domain": "arithmetic",
          "task_family": "f8_arithmetic_hallucination",
          "systems": [
            {
              "system": "LLM",
              "response": "1101",
              "execution_time": 0.2783839702606201,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "1101",
              "execution_time": 0.24410176277160645,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "1101",
              "execution_time": 0.24619531631469727,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f8_002",
          "query_text": "Calculate: (456 + 789) \u00d7 2 - 300 \u00f7 5",
          "expected_answer": "2430",
          "domain": "arithmetic",
          "task_family": "f8_arithmetic_hallucination",
          "systems": [
            {
              "system": "LLM",
              "response": "2430",
              "execution_time": 0.23307299613952637,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "2430",
              "execution_time": 0.29800868034362793,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "2430",
              "execution_time": 0.33202290534973145,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f8_003",
          "query_text": "If 25% of X is 150, what is 75% of X?",
          "expected_answer": "450",
          "domain": "arithmetic",
          "task_family": "f8_arithmetic_hallucination",
          "systems": [
            {
              "system": "LLM",
              "response": "450",
              "execution_time": 0.28532886505126953,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "450",
              "execution_time": 0.3039970397949219,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "450",
              "execution_time": 0.33289027214050293,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f8_004",
          "query_text": "A store marks up items by 40%, then offers a 30% discount. What is the effective price change on a $100 item?",
          "expected_answer": "98",
          "domain": "arithmetic",
          "task_family": "f8_arithmetic_hallucination",
          "systems": [
            {
              "system": "LLM",
              "response": "",
              "execution_time": 0.4028952121734619,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "",
              "execution_time": 0.41753172874450684,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "The final",
              "execution_time": 0.5284926891326904,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f8_005",
          "query_text": "What is 15% of 80 plus 20% of 60 minus 10% of 40?",
          "expected_answer": "20",
          "domain": "arithmetic",
          "task_family": "f8_arithmetic_hallucination",
          "systems": [
            {
              "system": "LLM",
              "response": "20",
              "execution_time": 0.36554384231567383,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "20",
              "execution_time": 0.4098939895629883,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "20",
              "execution_time": 0.29935479164123535,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        }
      ],
      "accuracy": {
        "LLM": 0.6,
        "LLM+RAG": 0.6,
        "RCE-LLM": 0.6
      }
    },
    "f9_noisy_rag": {
      "task_family": "f9_noisy_rag",
      "total_queries": 5,
      "queries": [
        {
          "query_id": "f9_001",
          "query_text": "What is the population of Mars in 2024?",
          "expected_answer": "0",
          "domain": "factual",
          "task_family": "f9_noisy_rag",
          "systems": [
            {
              "system": "LLM",
              "response": "0",
              "execution_time": 0.3131251335144043,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "0",
              "execution_time": 0.41057372093200684,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "0",
              "execution_time": 37.221635818481445,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f9_002",
          "query_text": "Who won the Nobel Prize in Physics in 2030?",
          "expected_answer": "unknown",
          "domain": "factual",
          "task_family": "f9_noisy_rag",
          "systems": [
            {
              "system": "LLM",
              "response": "I\u2019m sorry, but I don\u2019t have information about the Nobel Prize in Physics for 2030.",
              "execution_time": 0.9308490753173828,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "I\u2019m sorry, but I don\u2019t have information about the Nobel Prize in Physics for 2030.",
              "execution_time": 0.7761020660400391,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "I\u2019m sorry, but I don\u2019t have information about the Nobel Prize in Physics for 2030.",
              "execution_time": 0.539362907409668,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f9_003",
          "query_text": "What is the exact height of Mount Everest in nanometers?",
          "expected_answer": "8848870000000",
          "domain": "units",
          "task_family": "f9_noisy_rag",
          "systems": [
            {
              "system": "LLM",
              "response": "",
              "execution_time": 1.2467868328094482,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "",
              "execution_time": 1.0140368938446045,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "",
              "execution_time": 0.8291721343994141,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f9_004",
          "query_text": "According to the Constitution, what is the minimum age to become President of Mars?",
          "expected_answer": "undefined",
          "domain": "factual",
          "task_family": "f9_noisy_rag",
          "systems": [
            {
              "system": "LLM",
              "response": "There is no real Constitution of Mars, so no official minimum age is defined.",
              "execution_time": 1.0637853145599365,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "",
              "execution_time": 0.48760485649108887,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "There is no real Constitution of Mars, so no official minimum age is defined.",
              "execution_time": 0.4756169319152832,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f9_005",
          "query_text": "How many moons does Earth have that are larger than 1000km in diameter?",
          "expected_answer": "1",
          "domain": "factual",
          "task_family": "f9_noisy_rag",
          "systems": [
            {
              "system": "LLM",
              "response": "1",
              "execution_time": 0.2824678421020508,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "1",
              "execution_time": 0.5188498497009277,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "1",
              "execution_time": 0.8342227935791016,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        }
      ],
      "accuracy": {
        "LLM": 0.4,
        "LLM+RAG": 0.4,
        "RCE-LLM": 0.4
      }
    },
    "f10_confidence_calibration": {
      "task_family": "f10_confidence_calibration",
      "total_queries": 5,
      "queries": [
        {
          "query_id": "f10_001",
          "query_text": "The trophy doesn't fit in the suitcase because it is too large. What is too large?",
          "expected_answer": "the trophy",
          "domain": "coreference",
          "task_family": "f10_confidence_calibration",
          "systems": [
            {
              "system": "LLM",
              "response": "the trophy",
              "execution_time": 0.6815259456634521,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "The trophy.",
              "execution_time": 0.5743308067321777,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "the trophy",
              "execution_time": 0.38270092010498047,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f10_002",
          "query_text": "The city councilmen refused the demonstrators a permit because they feared violence. Who feared violence?",
          "expected_answer": "the councilmen",
          "domain": "coreference",
          "task_family": "f10_confidence_calibration",
          "systems": [
            {
              "system": "LLM",
              "response": "The city councilmen.",
              "execution_time": 0.2840399742126465,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": false
            },
            {
              "system": "LLM+RAG",
              "response": "The city councilmen feared violence.",
              "execution_time": 0.7008991241455078,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "The city councilmen.",
              "execution_time": 0.533933162689209,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": false
            }
          ]
        },
        {
          "query_id": "f10_003",
          "query_text": "A bat and ball cost $1.10 total. The bat costs $1.00 more than the ball. How much does the ball cost?",
          "expected_answer": "0.05",
          "domain": "arithmetic",
          "task_family": "f10_confidence_calibration",
          "systems": [
            {
              "system": "LLM",
              "response": "$0.05",
              "execution_time": 0.35292601585388184,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "",
              "execution_time": 0.6522471904754639,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": false
            },
            {
              "system": "RCE-LLM",
              "response": "$0.05",
              "execution_time": 0.5596649646759033,
              "success": true,
              "coherence_score": 0.9466666666666668,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f10_004",
          "query_text": "If 5 machines take 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?",
          "expected_answer": "5",
          "domain": "arithmetic",
          "task_family": "f10_confidence_calibration",
          "systems": [
            {
              "system": "LLM",
              "response": "5 minutes.",
              "execution_time": 0.6902899742126465,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "5 minutes.",
              "execution_time": 0.5866990089416504,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "5 minutes.",
              "execution_time": 0.7065892219543457,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        },
        {
          "query_id": "f10_005",
          "query_text": "In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to cover half the lake?",
          "expected_answer": "47",
          "domain": "arithmetic",
          "task_family": "f10_confidence_calibration",
          "systems": [
            {
              "system": "LLM",
              "response": "47 days",
              "execution_time": 0.3875138759613037,
              "success": true,
              "coherence_score": null,
              "error": null,
              "correct": true
            },
            {
              "system": "LLM+RAG",
              "response": "47 days.",
              "execution_time": 0.3305490016937256,
              "success": true,
              "coherence_score": null,
              "retrieval_enabled": true,
              "error": null,
              "correct": true
            },
            {
              "system": "RCE-LLM",
              "response": "47 days",
              "execution_time": 0.5274660587310791,
              "success": true,
              "coherence_score": 1.0,
              "coherence_modules": [
                "units",
                "temporal",
                "arithmetic",
                "coreference",
                "entailment"
              ],
              "pipeline_trace": null,
              "error": null,
              "correct": true
            }
          ]
        }
      ],
      "accuracy": {
        "LLM": 0.8,
        "LLM+RAG": 0.6,
        "RCE-LLM": 0.8
      }
    }
  }
}