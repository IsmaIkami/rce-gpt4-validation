{
  "task_family": "f6_contradictory_reasoning",
  "total_queries": 5,
  "queries": [
    {
      "query_id": "f6_001",
      "query_text": "If it's not true that the medication doesn't work, does the medication work?",
      "expected_answer": "yes",
      "domain": "logic",
      "task_family": "f6_contradictory_reasoning",
      "systems": [
        {
          "system": "LLM",
          "response": "yes",
          "execution_time": 0.5557019710540771,
          "success": true,
          "coherence_score": null,
          "error": null,
          "correct": true
        },
        {
          "system": "LLM+RAG",
          "response": "yes",
          "execution_time": 0.38887882232666016,
          "success": true,
          "coherence_score": null,
          "retrieval_enabled": true,
          "error": null,
          "correct": true
        },
        {
          "system": "RCE-LLM",
          "response": "yes",
          "execution_time": 0.43483781814575195,
          "success": true,
          "coherence_score": 1.0,
          "coherence_modules": [
            "units",
            "temporal",
            "arithmetic",
            "coreference",
            "entailment"
          ],
          "pipeline_trace": null,
          "error": null,
          "correct": true
        }
      ]
    },
    {
      "query_id": "f6_002",
      "query_text": "A report states a sample weighs 500 grams. Later it says 0.5 kilograms. Is this contradictory?",
      "expected_answer": "no",
      "domain": "units",
      "task_family": "f6_contradictory_reasoning",
      "systems": [
        {
          "system": "LLM",
          "response": "No.",
          "execution_time": 0.2816121578216553,
          "success": true,
          "coherence_score": null,
          "error": null,
          "correct": true
        },
        {
          "system": "LLM+RAG",
          "response": "No.",
          "execution_time": 0.40953707695007324,
          "success": true,
          "coherence_score": null,
          "retrieval_enabled": true,
          "error": null,
          "correct": true
        },
        {
          "system": "RCE-LLM",
          "response": "No.",
          "execution_time": 0.3505058288574219,
          "success": true,
          "coherence_score": 1.0,
          "coherence_modules": [
            "units",
            "temporal",
            "arithmetic",
            "coreference",
            "entailment"
          ],
          "pipeline_trace": null,
          "error": null,
          "correct": true
        }
      ]
    },
    {
      "query_id": "f6_003",
      "query_text": "Statement 1: The drug reduces fever. Statement 2: The medication doesn't increase body temperature. Are these consistent?",
      "expected_answer": "yes",
      "domain": "medical",
      "task_family": "f6_contradictory_reasoning",
      "systems": [
        {
          "system": "LLM",
          "response": "yes",
          "execution_time": 0.2941780090332031,
          "success": true,
          "coherence_score": null,
          "error": null,
          "correct": true
        },
        {
          "system": "LLM+RAG",
          "response": "",
          "execution_time": 0.40267086029052734,
          "success": true,
          "coherence_score": null,
          "retrieval_enabled": true,
          "error": null,
          "correct": false
        },
        {
          "system": "RCE-LLM",
          "response": "yes",
          "execution_time": 0.411470890045166,
          "success": true,
          "coherence_score": 0.25,
          "coherence_modules": [
            "units",
            "temporal",
            "arithmetic",
            "coreference",
            "entailment"
          ],
          "pipeline_trace": null,
          "error": null,
          "correct": true
        }
      ]
    },
    {
      "query_id": "f6_004",
      "query_text": "If it isn't the case that the system isn't secure, is the system secure?",
      "expected_answer": "yes",
      "domain": "logic",
      "task_family": "f6_contradictory_reasoning",
      "systems": [
        {
          "system": "LLM",
          "response": "yes",
          "execution_time": 0.4866039752960205,
          "success": true,
          "coherence_score": null,
          "error": null,
          "correct": true
        },
        {
          "system": "LLM+RAG",
          "response": "",
          "execution_time": 0.515355110168457,
          "success": true,
          "coherence_score": null,
          "retrieval_enabled": true,
          "error": null,
          "correct": false
        },
        {
          "system": "RCE-LLM",
          "response": "yes",
          "execution_time": 0.4317891597747803,
          "success": true,
          "coherence_score": 0.5,
          "coherence_modules": [
            "units",
            "temporal",
            "arithmetic",
            "coreference",
            "entailment"
          ],
          "pipeline_trace": null,
          "error": null,
          "correct": true
        }
      ]
    },
    {
      "query_id": "f6_005",
      "query_text": "Document says: Temperature is 25\u00b0C. Later: Temperature is 77\u00b0F. Is there a contradiction?",
      "expected_answer": "no",
      "domain": "units",
      "task_family": "f6_contradictory_reasoning",
      "systems": [
        {
          "system": "LLM",
          "response": "no",
          "execution_time": 0.38352012634277344,
          "success": true,
          "coherence_score": null,
          "error": null,
          "correct": true
        },
        {
          "system": "LLM+RAG",
          "response": "No.",
          "execution_time": 0.40950512886047363,
          "success": true,
          "coherence_score": null,
          "retrieval_enabled": true,
          "error": null,
          "correct": true
        },
        {
          "system": "RCE-LLM",
          "response": "no",
          "execution_time": 0.43251895904541016,
          "success": true,
          "coherence_score": 1.0,
          "coherence_modules": [
            "units",
            "temporal",
            "arithmetic",
            "coreference",
            "entailment"
          ],
          "pipeline_trace": null,
          "error": null,
          "correct": true
        }
      ]
    }
  ],
  "accuracy": {
    "LLM": 1.0,
    "LLM+RAG": 0.6,
    "RCE-LLM": 1.0
  }
}